{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UT-whgM5YV8X",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Language Modeling\n",
    "\n",
    "A language model attempts to approximate the underlying statistics of a text corpus $P(tok_n | tok_1, tok_2, ..., tok_{n-1}; \\theta)$ where $\\theta$ is a set of learned parameters/weights. For the purposes of this notebook, tokens will be words. Language models can be used for a variety of applications, one of which being text generation. In this assignement we will be looking at Recurrent Neural Networks.\n",
    "\n",
    "**Tips:**\n",
    "- Read all the code. We don't ask you to write the training loops, evaluation loops, and generation loops, but it is often instructive to see how the models are trained and evaluated.\n",
    "- If you have a model that is learning (loss is decreasing), but you want to increase accuracy, try using ``nn.Dropout`` layers just before the final linear layer to force the model to handle missing or unfamiliar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# start time - notebook execution\n",
    "import time\n",
    "start_nb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VeQWnSY8CgT1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "vcoLFG32u68g",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "id": "us-v9ZxoTHV1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Initialize the Autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# import the autograder tests\n",
    "import hw3a_tests as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "Fp1_pgBuY5zT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will build a *vocabulary*, which will act as a dictionary of all the words our systems will know about. It will also allow us to map words to tokens, which will be unique indexes in the vocabulary. This will further allow us to transform words into one-hot vectors, where a word is represented as a vector of the same length as the vocabulary wherein all values are zeros except for the *i*th element, where *i* is the token number of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "id": "1ybnRLpOTYwi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "SOS_token = 0    # A special token representing the start of a sequence\n",
    "EOS_token = 1    # A special token representing the end of a sequence\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name                             # The name of the vocabulary\n",
    "        self._word2index = {}                        # Map words to token index\n",
    "        self._word2count = {}                        # Track how many times a word occurs in a corpus\n",
    "        self._index2word = {0: \"SOS\", 1: \"EOS\"}      # Map token indexs back into words\n",
    "        self._n_words = 2 # Count SOS and EOS        # Number of unique words in the corpus\n",
    "\n",
    "    # Get a list of all words\n",
    "    def get_words(self):\n",
    "      return list(self._word2count.keys())\n",
    "\n",
    "    # Get the number of words\n",
    "    def num_words(self):\n",
    "      return self._n_words\n",
    "\n",
    "    # Convert a word into a token index\n",
    "    def word2index(self, word):\n",
    "      return self._word2index[word]\n",
    "\n",
    "    # Convert a token into a word\n",
    "    def index2word(self, word):\n",
    "      return self._index2word[word]\n",
    "\n",
    "    # Get the number of times a word occurs\n",
    "    def word2count(self, word):\n",
    "      return self._word2count[word]\n",
    "\n",
    "    # Add all the words in a sentence to the vocabulary\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    # Add a single word to the vocabulary\n",
    "    def add_word(self, word):\n",
    "        if word not in self._word2index:\n",
    "            self._word2index[word] = self._n_words\n",
    "            self._word2count[word] = 1\n",
    "            self._index2word[self._n_words] = word\n",
    "            self._n_words += 1\n",
    "        else:\n",
    "            self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "8L7cXWAYacx3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "These are some helper functions to *normalize* texts, ie, make the text regular and remove some of the more problematic exceptions found in texts. This normalizer will make all words lowercase, trim plurals, and remove non-letter characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": false,
    "id": "SpAin8FuTsNr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert any unicode to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "0G1p53AuazDJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Download a corpus. This corpus is the ascii text of the book, *The Silmarillion*, by J.R.R. Tolkein. It has a lot of non-common words and names to illustrate how language models deal with such things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "id": "i6IA64U3T3K7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.txt already downloaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('data.txt'):\n",
    "  print(\"data.txt already downloaded\")\n",
    "else:\n",
    "  print(\"downloading data.txt\")\n",
    "  !wget -O data.txt https://www.dropbox.com/s/pgvn1n7t4sjxt8r/silmarillion?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "EEhv3LTxb1If",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's read in the data and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": false,
    "id": "AfzYspyHUT7k",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Silmarillon Chapter 1\\n\\n\\nOf the Beginning of Days It is told among the wise that the First War began before Arda was full-shaped, and ere yet there was any thing that grew or walked upon earth; and for long Melkor had the upper hand. But in the midst of the war a spirit of great strength and hardihood came to the aid of the Valar, hearing in the far heaven that there was battle in the Little Kingdom; and Arda was filled with the sound of his laughter. So came Tulkas the Strong, whose anger passes like a mighty wind, scattering cloud and darkness before it; and Melkor fled before his wrath and his laughter, and forsook Arda, and there was peace for a long age. And Tulkas remained and became one of the Valar of the Kingdom of Arda; but Melkor brooded in the outer darkness, and his hate was given to Tulkas for ever after.\\n\\nIn that time the Valar brought order to the seas and the lands and the mountains, and Yavanna planted at last the seeds that she had long devised. And since, when th'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'data.txt'\n",
    "with open(filename, encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "rPga6_LmcDWJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Normalize the text and build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "id": "FV_ib2sZYMqo",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "normalized_text = normalize_string(text)\n",
    "VOCAB = Vocab(\"text\")\n",
    "VOCAB.add_sentence(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VBei0ecBcHAy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Make training and testing data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": false,
    "id": "HMc-3xncyc_Q",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 tokens\n",
      "[ 2  3  4  5  2  6  5  7  8  9 10 11  2 12 13  2 14 15 16 17 18 19 20 21\n",
      " 22 23 24 25 19 26 27 13 28 29 30 31 32 22 33 34 35 36  2 37 38 39 40 41\n",
      "  2 42  5  2 15 43 44  5 45 46 22 47 48 49  2 50  5  2 51 52 41  2 53 54\n",
      " 13 25 19 55 41  2 56 57 22 18 19 58 59  2 60  5 61 62 39 63 48 64  2 65\n",
      " 66 67 68 69]\n"
     ]
    }
   ],
   "source": [
    "# Convert every word into a token and build a numpy array of tokens\n",
    "encoded_text = np.array([VOCAB.word2index(word) for word in normalized_text.split()])\n",
    "print(\"The first 100 tokens\")\n",
    "print(encoded_text[:100])\n",
    "# get the validation and the training data\n",
    "test_split = 0.1\n",
    "test_idx = int(len(encoded_text) * (1 - test_split))\n",
    "TRAIN = encoded_text[:test_idx]\n",
    "TEST = encoded_text[test_idx:]\n",
    "# Decrease the size of the training set to make the assignment more tractable\n",
    "TRAIN = TRAIN[:len(TRAIN)//10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "niQcV2wIFq29",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# RNN\n",
    "\n",
    "**Complete the code for an RNN.** An RNN takes a one-hot vector for a single word and a hidden state vector (initially all zeroes), compresses it to a hidden state, and then decompresses it. The decompressed word is tested against the **next** word in the training sequence using cross-entropy loss. The RNN also produces the hidden state vector to pass in with the next word in the training sequence. The neural network must guess the next word as well as learn to create a hidden state vector that helps the next iteration make a better word guess.\n",
    "\n",
    "The neural network's forward function should take two inputs:\n",
    "- The input word (`x`) represented as a one-hot vector of size `1 x vocab_size`\n",
    "- The hidden state, a `1 x hidden_size` vector.\n",
    "\n",
    "A brief note on batching: We will not be using batching in this assignment. But there must always be a batching dimension in our input and output tensors. Thus we will have a batching dimension size of 1 and our tensors will often be of a shape `1 x something`.\n",
    "\n",
    "The neural network architecture shoud concatenate the `x` and the `hidden_state` to make one big long vector. The neural network get's to learn through weights whether to draw from the one-hot (or certain parts of the one-hot) or the hidden state when trying to predict the next token.\n",
    "\n",
    "The neural network should have two affine transformations (`nn.Linear` modules). The first should transform the a tensor of size `1 x (vocab_size + hidden_size)` into a tensor of size `1 x hidden_size`, followed by a sigmoid activation. This compresses the information from the input and forces the neural network to make compromises about what is important. Think of the sigmoid a gate that says yes or no to different combinations of inputs. The second affine transform should be from a tensor of `1 x hidden_size` to one of `1 x vocab_size`. The values in resultant tensor are the raw scores for each possible token in the vocabulary. The forward function should run these scores through a log softmax.\n",
    "\n",
    "The output of the forward function should be two values: a tensor of log-scale scores for each token, and a new hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "y0r-3PvoRV3S",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RNN---Model (30 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "id": "vhb3w8CUqydH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ef3f98851d99506",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size=None):\n",
    "    super(MyRNN, self).__init__()\n",
    "    # If output_size is not given, use input_size\n",
    "    if output_size is None:\n",
    "      output_size = input_size\n",
    "    self.input_size = input_size      # the size of the input vocabulary\n",
    "    self.hidden_size = hidden_size    # the size of the hidden state\n",
    "    self.output_size = output_size    # the size of the output vocabulary (if different)\n",
    "    \n",
    "    # Create 2 affine transformation as instructed.\n",
    "    # First linear layer (vocab+hidden) to (hidden)\n",
    "    self.enc_fc = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "    # Second liniear layer (hidden) to output\n",
    "    self.dec_fc = nn.Linear(hidden_size, output_size)\n",
    "    # Activation\n",
    "    self.enc_act = nn.Sigmoid()\n",
    "    self.dec_act = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x, hidden_state):\n",
    "    output = None\n",
    "    hidden = None\n",
    "\n",
    "    # Concatenate input\n",
    "    input_state = torch.cat((x,hidden_state), dim = 1)      # (1, vocab + hidden)\n",
    "    # Collect new hidden state\n",
    "    hidden = self.enc_act(self.enc_fc(input_state))  # (1, hidden size)\n",
    "    # Decoder\n",
    "    output = self.dec_act(self.dec_fc(hidden))       # (1, vocab size)\n",
    "\n",
    "\n",
    "    return output, hidden\n",
    "\n",
    "  # Make an initial hidden state with some randomness to the values\n",
    "  def init_hidden(self):\n",
    "    return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "QNlqo68hdXT8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Construct the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "id": "BimhoCMSSgRd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# it's ok to modify this cell\n",
    "RNN_HIDDEN_SIZE = 256\n",
    "RNN_LEARNING_RATE = 0.0005\n",
    "RNN_NUM_EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "id": "xVQa8yuJq9Sv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the network\n",
    "rnn = MyRNN(VOCAB.num_words(), RNN_HIDDEN_SIZE)\n",
    "\n",
    "# Create the loss function and optimizer\n",
    "criterion_rnn = nn.NLLLoss()\n",
    "optimizer_rnn = torch.optim.Adam(rnn.parameters(), lr=RNN_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "id": "M6o9RfTm4A5q",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NllLossBackward0\n",
      "LogSoftmaxBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "SigmoidBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "NllLossBackward0\n",
      "LogSoftmaxBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "SigmoidBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "NllLossBackward0\n",
      "LogSoftmaxBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "SigmoidBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "Test A: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value of 10 to receive full credit (no partial credit)\n",
    "ag.unit_test_RNN_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function.\n",
    "\n",
    "`token2onehot()` takes a token—a number—and converts it to a one-hot tensor of the shape `1 x vocab_size`. All values should be zeros except for one element, which should be a `1`.\n",
    "\n",
    "`get_rnn_x_y_()` should return the `x` and `y` for a recurrent neural network.\n",
    "\n",
    "- The `x` return value should be a tensor containing a one-hot vector representing the word at position `index` and have a shape of `1 x vocab_size` (the batch size is 1).\n",
    "- The `y` return value should be the token of the word at position `index+1` and be a vector with a single value in it. That is, it should not be a scalar but a vector of length 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2onehot(token, vocab_size = VOCAB.num_words()):\n",
    "  one_hot = None\n",
    "  \n",
    "  # print(token.shape)\n",
    "  one_hot = torch.zeros(1, vocab_size)\n",
    "  one_hot[0, token] = 1.0\n",
    "\n",
    "  \n",
    "  return one_hot\n",
    "\n",
    "def get_rnn_x_y(data, index, vocab_size = VOCAB.num_words()):\n",
    "  x = None\n",
    "  y = None\n",
    "  \n",
    "  x = data[index]\n",
    "  y = data[index + 1]\n",
    "  x = token2onehot(x, vocab_size)\n",
    "  y = torch.tensor([y], dtype=torch.long)\n",
    "\n",
    "  \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test B: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value of 10 to receive full credit (no partial credit)\n",
    "ag.unit_test_token2onehot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test C: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value of 100 to receive full credit (no partial credit)\n",
    "ag.unit_test_get_xy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "RDFRH_FUj_4F",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following is the training loop. You can see how your `get_rnn_x_y()` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": false,
    "id": "97XvkPmsuC2Q",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def train_rnn(model, optimizer, criterion, data, num_epochs):\n",
    "  model.train()\n",
    "  for epoch in range(num_epochs):\n",
    "    hidden_state = model.init_hidden()\n",
    "    losses = []\n",
    "    for i in range(len(data)-2):\n",
    "      x, y = get_rnn_x_y(data, i)\n",
    "      x = x.float()\n",
    "      output, new_hidden = model(x, hidden_state)\n",
    "      hidden_state = new_hidden.detach()\n",
    "      loss = criterion(output, y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      losses.append(loss.item())\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "      optimizer.step()\n",
    "      if i%100 == 0:\n",
    "        print('iter', i, 'loss', np.array(losses).mean())\n",
    "    print('epoch', epoch, 'loss', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "id": "kyvFAzHzvmJz",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 loss 8.599178314208984\n",
      "iter 100 loss 7.770304787276995\n",
      "iter 200 loss 7.115117769336226\n",
      "iter 300 loss 6.9720861824643965\n",
      "iter 400 loss 6.904005127951986\n",
      "iter 500 loss 6.865162016388899\n",
      "iter 600 loss 6.85215837090662\n",
      "iter 700 loss 6.907519641684398\n",
      "iter 800 loss 6.863512961159038\n",
      "iter 900 loss 6.882670731047018\n",
      "iter 1000 loss 6.838692603649555\n",
      "iter 1100 loss 6.759765257302682\n",
      "iter 1200 loss 6.769200443824463\n",
      "iter 1300 loss 6.773051324392813\n",
      "iter 1400 loss 6.804413663924038\n",
      "iter 1500 loss 6.808048483056279\n",
      "iter 1600 loss 6.824075170861863\n",
      "iter 1700 loss 6.8038947246552635\n",
      "iter 1800 loss 6.797307993755415\n",
      "iter 1900 loss 6.835429075829046\n",
      "iter 2000 loss 6.8563809046919255\n",
      "iter 2100 loss 6.859360862379696\n",
      "iter 2200 loss 6.825062740991896\n",
      "iter 2300 loss 6.800405086666954\n",
      "iter 2400 loss 6.797861463325513\n",
      "iter 2500 loss 6.792690817283088\n",
      "iter 2600 loss 6.775830049094949\n",
      "iter 2700 loss 6.763777700542124\n",
      "iter 2800 loss 6.75262380612403\n",
      "iter 2900 loss 6.752545065240422\n",
      "iter 3000 loss 6.766550077910583\n",
      "iter 3100 loss 6.758296402182666\n",
      "iter 3200 loss 6.767764008108506\n",
      "iter 3300 loss 6.763692252674236\n",
      "iter 3400 loss 6.774320947356309\n",
      "iter 3500 loss 6.792148269097895\n",
      "iter 3600 loss 6.817436392126531\n",
      "iter 3700 loss 6.837745779915392\n",
      "iter 3800 loss 6.851648249239772\n",
      "iter 3900 loss 6.864644438173489\n",
      "iter 4000 loss 6.867283807906828\n",
      "iter 4100 loss 6.854868126566658\n",
      "iter 4200 loss 6.873811649044648\n",
      "iter 4300 loss 6.894584926331606\n",
      "iter 4400 loss 6.913572058085013\n",
      "iter 4500 loss 6.922341843070891\n",
      "iter 4600 loss 6.936750690047313\n",
      "iter 4700 loss 6.952041096546325\n",
      "iter 4800 loss 6.957121757089384\n",
      "iter 4900 loss 6.950329083200621\n",
      "iter 5000 loss 6.949033802853801\n",
      "iter 5100 loss 6.961278966959026\n",
      "iter 5200 loss 6.967961441345156\n",
      "iter 5300 loss 6.968193714179266\n",
      "iter 5400 loss 6.989302374570155\n",
      "iter 5500 loss 6.992340311183212\n",
      "iter 5600 loss 6.98131549036986\n",
      "iter 5700 loss 6.9760534905316725\n",
      "iter 5800 loss 6.987597340593007\n",
      "iter 5900 loss 6.987550143002857\n",
      "iter 6000 loss 6.990983277435125\n",
      "iter 6100 loss 6.97967482515875\n",
      "iter 6200 loss 6.97013538246327\n",
      "iter 6300 loss 6.9629516256478\n",
      "iter 6400 loss 6.961323494725703\n",
      "iter 6500 loss 6.9756875279096064\n",
      "iter 6600 loss 6.965176585153673\n",
      "iter 6700 loss 6.957038490314338\n",
      "iter 6800 loss 6.9585245766546455\n",
      "iter 6900 loss 6.947709688568889\n",
      "iter 7000 loss 6.9380942654258915\n",
      "iter 7100 loss 6.930462172947674\n",
      "iter 7200 loss 6.932325084215071\n",
      "iter 7300 loss 6.929351699775155\n",
      "iter 7400 loss 6.932824279356447\n",
      "iter 7500 loss 6.9335881969067685\n",
      "iter 7600 loss 6.9335431514986405\n",
      "iter 7700 loss 6.933688010802255\n",
      "iter 7800 loss 6.928610074582765\n",
      "iter 7900 loss 6.917825601626282\n",
      "iter 8000 loss 6.910959137661608\n",
      "iter 8100 loss 6.89519746157729\n",
      "iter 8200 loss 6.8877486916536705\n",
      "iter 8300 loss 6.891581053102524\n",
      "iter 8400 loss 6.889883763753804\n",
      "iter 8500 loss 6.88142531991019\n",
      "iter 8600 loss 6.883862935107969\n",
      "iter 8700 loss 6.875687907117768\n",
      "iter 8800 loss 6.873179715569081\n",
      "iter 8900 loss 6.868279721131286\n",
      "iter 9000 loss 6.869557382583406\n",
      "iter 9100 loss 6.862352540133851\n",
      "iter 9200 loss 6.85779495553962\n",
      "iter 9300 loss 6.85134356511612\n",
      "iter 9400 loss 6.853804416123153\n",
      "iter 9500 loss 6.851793188099459\n",
      "iter 9600 loss 6.852689462140902\n",
      "iter 9700 loss 6.8509140255930205\n",
      "iter 9800 loss 6.85834798946294\n",
      "iter 9900 loss 6.857645053554576\n",
      "iter 10000 loss 6.857179132744903\n",
      "iter 10100 loss 6.857115053679728\n",
      "iter 10200 loss 6.851968099820638\n",
      "iter 10300 loss 6.847993268652826\n",
      "iter 10400 loss 6.838749626980024\n",
      "iter 10500 loss 6.839691167285971\n",
      "iter 10600 loss 6.838104224949443\n",
      "iter 10700 loss 6.8260220759227535\n",
      "iter 10800 loss 6.826992784867782\n",
      "iter 10900 loss 6.821216742737164\n",
      "iter 11000 loss 6.817655420831069\n",
      "iter 11100 loss 6.815011929546474\n",
      "iter 11200 loss 6.812803326754812\n",
      "iter 11300 loss 6.806114003306184\n",
      "iter 11400 loss 6.809947975185731\n",
      "iter 11500 loss 6.8114184599945355\n",
      "epoch 0 loss 6.809622421143347\n",
      "iter 0 loss 20.977985382080078\n",
      "iter 100 loss 5.822015195789904\n",
      "iter 200 loss 5.630094689812826\n",
      "iter 300 loss 5.572309981350883\n",
      "iter 400 loss 5.649389292979776\n",
      "iter 500 loss 5.662335985553955\n",
      "iter 600 loss 5.646015773497881\n",
      "iter 700 loss 5.611820104884013\n",
      "iter 800 loss 5.581324379467935\n",
      "iter 900 loss 5.592387868019109\n",
      "iter 1000 loss 5.586138031520806\n",
      "iter 1100 loss 5.566028732424103\n",
      "iter 1200 loss 5.546392339899776\n",
      "iter 1300 loss 5.5189910859075715\n",
      "iter 1400 loss 5.5367064187137\n",
      "iter 1500 loss 5.542948591518529\n",
      "iter 1600 loss 5.5690419957981785\n",
      "iter 1700 loss 5.561430288545529\n",
      "iter 1800 loss 5.575432121555121\n",
      "iter 1900 loss 5.608727032388279\n",
      "iter 2000 loss 5.622670130274523\n",
      "iter 2100 loss 5.629733385386097\n",
      "iter 2200 loss 5.613254971771769\n",
      "iter 2300 loss 5.600697903180319\n",
      "iter 2400 loss 5.599056611702572\n",
      "iter 2500 loss 5.602095754396339\n",
      "iter 2600 loss 5.59308343979085\n",
      "iter 2700 loss 5.586365054996311\n",
      "iter 2800 loss 5.582279240753088\n",
      "iter 2900 loss 5.579927749117667\n",
      "iter 3000 loss 5.579904680399448\n",
      "iter 3100 loss 5.580739312712433\n",
      "iter 3200 loss 5.590548338116575\n",
      "iter 3300 loss 5.5921690716991925\n",
      "iter 3400 loss 5.602059947378809\n",
      "iter 3500 loss 5.608930144462201\n",
      "iter 3600 loss 5.6200814687669425\n",
      "iter 3700 loss 5.6271088783385395\n",
      "iter 3800 loss 5.641619890261813\n",
      "iter 3900 loss 5.659385554639537\n",
      "iter 4000 loss 5.659420003059118\n",
      "iter 4100 loss 5.651792930934639\n",
      "iter 4200 loss 5.663192656941994\n",
      "iter 4300 loss 5.680242463866657\n",
      "iter 4400 loss 5.691011738571301\n",
      "iter 4500 loss 5.702727986028315\n",
      "iter 4600 loss 5.714458086858027\n",
      "iter 4700 loss 5.727958748871102\n",
      "iter 4800 loss 5.735076430724875\n",
      "iter 4900 loss 5.734796096088691\n",
      "iter 5000 loss 5.737315410448298\n",
      "iter 5100 loss 5.7464672787777\n",
      "iter 5200 loss 5.75384147187605\n",
      "iter 5300 loss 5.755847259144405\n",
      "iter 5400 loss 5.771671010363744\n",
      "iter 5500 loss 5.781417971945875\n",
      "iter 5600 loss 5.778586371235285\n",
      "iter 5700 loss 5.781511052253685\n",
      "iter 5800 loss 5.790774838890706\n",
      "iter 5900 loss 5.790989860026155\n",
      "iter 6000 loss 5.7946335608274655\n",
      "iter 6100 loss 5.79043364000211\n",
      "iter 6200 loss 5.788246573613279\n",
      "iter 6300 loss 5.791497252882709\n",
      "iter 6400 loss 5.79260567101142\n",
      "iter 6500 loss 5.811186434919698\n",
      "iter 6600 loss 5.803805421932306\n",
      "iter 6700 loss 5.804058287432257\n",
      "iter 6800 loss 5.806000412807554\n",
      "iter 6900 loss 5.802326328533143\n",
      "iter 7000 loss 5.798475467248604\n",
      "iter 7100 loss 5.7991885348971906\n",
      "iter 7200 loss 5.802672671197339\n",
      "iter 7300 loss 5.803449340535098\n",
      "iter 7400 loss 5.806358179648556\n",
      "iter 7500 loss 5.812605435149604\n",
      "iter 7600 loss 5.816355130826466\n",
      "iter 7700 loss 5.819042934599426\n",
      "iter 7800 loss 5.819369981115773\n",
      "iter 7900 loss 5.812336811757875\n",
      "iter 8000 loss 5.808312584829843\n",
      "iter 8100 loss 5.797406120588383\n",
      "iter 8200 loss 5.79578575035749\n",
      "iter 8300 loss 5.802148711718525\n",
      "iter 8400 loss 5.8043435489486805\n",
      "iter 8500 loss 5.803020483979497\n",
      "iter 8600 loss 5.808553799256594\n",
      "iter 8700 loss 5.8032425125336635\n",
      "iter 8800 loss 5.804347396517441\n",
      "iter 8900 loss 5.801002728582976\n",
      "iter 9000 loss 5.804454509612952\n",
      "iter 9100 loss 5.8020783724792775\n",
      "iter 9200 loss 5.800620054900841\n",
      "iter 9300 loss 5.7985517741532915\n",
      "iter 9400 loss 5.802775632266306\n",
      "iter 9500 loss 5.8026974541956085\n",
      "iter 9600 loss 5.806494503841619\n",
      "iter 9700 loss 5.807601508502339\n",
      "iter 9800 loss 5.816283816760376\n",
      "iter 9900 loss 5.817679898783078\n",
      "iter 10000 loss 5.820195522708763\n",
      "iter 10100 loss 5.821848303859257\n",
      "iter 10200 loss 5.820894394102412\n",
      "iter 10300 loss 5.822159484713769\n",
      "iter 10400 loss 5.81928313579978\n",
      "iter 10500 loss 5.8242466357479765\n",
      "iter 10600 loss 5.825297164089494\n",
      "iter 10700 loss 5.818498065988463\n",
      "iter 10800 loss 5.821250940347206\n",
      "iter 10900 loss 5.8203862849272685\n",
      "iter 11000 loss 5.819887172224527\n",
      "iter 11100 loss 5.821050319940445\n",
      "iter 11200 loss 5.822346007840041\n",
      "iter 11300 loss 5.819453255322436\n",
      "iter 11400 loss 5.821887408555307\n",
      "iter 11500 loss 5.826308336245937\n",
      "epoch 1 loss 5.825004981667565\n",
      "iter 0 loss 8.65027904510498\n",
      "iter 100 loss 5.480431623092972\n",
      "iter 200 loss 5.385317480682733\n",
      "iter 300 loss 5.40968212059566\n",
      "iter 400 loss 5.506485613950173\n",
      "iter 500 loss 5.523381999033892\n",
      "iter 600 loss 5.506505226731895\n",
      "iter 700 loss 5.481895335305604\n",
      "iter 800 loss 5.447481635060352\n",
      "iter 900 loss 5.472284801262994\n",
      "iter 1000 loss 5.47237251262803\n",
      "iter 1100 loss 5.4558121700052995\n",
      "iter 1200 loss 5.435120480642231\n",
      "iter 1300 loss 5.407996683259087\n",
      "iter 1400 loss 5.430716769704642\n",
      "iter 1500 loss 5.442024036834908\n",
      "iter 1600 loss 5.466882165449251\n",
      "iter 1700 loss 5.461611313269041\n",
      "iter 1800 loss 5.478113217065229\n",
      "iter 1900 loss 5.515138549750758\n",
      "iter 2000 loss 5.531572832800876\n",
      "iter 2100 loss 5.539001525200191\n",
      "iter 2200 loss 5.521713262969718\n",
      "iter 2300 loss 5.50689238089678\n",
      "iter 2400 loss 5.505796613021971\n",
      "iter 2500 loss 5.506539207799394\n",
      "iter 2600 loss 5.4953741535848035\n",
      "iter 2700 loss 5.4892789179287504\n",
      "iter 2800 loss 5.487646843186024\n",
      "iter 2900 loss 5.485949827967451\n",
      "iter 3000 loss 5.483352286990028\n",
      "iter 3100 loss 5.483722682601404\n",
      "iter 3200 loss 5.492717084331424\n",
      "iter 3300 loss 5.493474003984978\n",
      "iter 3400 loss 5.50073473986213\n",
      "iter 3500 loss 5.504941898404752\n",
      "iter 3600 loss 5.513450038491835\n",
      "iter 3700 loss 5.517934759096885\n",
      "iter 3800 loss 5.5307236648890195\n",
      "iter 3900 loss 5.545994419637964\n",
      "iter 4000 loss 5.544287178660744\n",
      "iter 4100 loss 5.535677646517288\n",
      "iter 4200 loss 5.545204077521638\n",
      "iter 4300 loss 5.561037926379677\n",
      "iter 4400 loss 5.570536433759941\n",
      "iter 4500 loss 5.582165425183561\n",
      "iter 4600 loss 5.59322257127497\n",
      "iter 4700 loss 5.604534650183034\n",
      "iter 4800 loss 5.609697722523964\n",
      "iter 4900 loss 5.608441226663237\n",
      "iter 5000 loss 5.610569769183986\n",
      "iter 5100 loss 5.619755437279585\n",
      "iter 5200 loss 5.626991721033088\n",
      "iter 5300 loss 5.628971852211767\n",
      "iter 5400 loss 5.642855669131237\n",
      "iter 5500 loss 5.6507267076342655\n",
      "iter 5600 loss 5.647899389296232\n",
      "iter 5700 loss 5.649309067012783\n",
      "iter 5800 loss 5.657732005924156\n",
      "iter 5900 loss 5.6567725919021665\n",
      "iter 6000 loss 5.658832638753075\n",
      "iter 6100 loss 5.653035468870822\n",
      "iter 6200 loss 5.649600827000426\n",
      "iter 6300 loss 5.652324782756279\n",
      "iter 6400 loss 5.653183989509663\n",
      "iter 6500 loss 5.670465006745883\n",
      "iter 6600 loss 5.6632423310243905\n",
      "iter 6700 loss 5.662831664559041\n",
      "iter 6800 loss 5.6648520023121485\n",
      "iter 6900 loss 5.661016547432448\n",
      "iter 7000 loss 5.656667405340079\n",
      "iter 7100 loss 5.656861014831001\n",
      "iter 7200 loss 5.658915647809298\n",
      "iter 7300 loss 5.659677275677296\n",
      "iter 7400 loss 5.661122044263748\n",
      "iter 7500 loss 5.6654603917123385\n",
      "iter 7600 loss 5.668051266300492\n",
      "iter 7700 loss 5.669647270157389\n",
      "iter 7800 loss 5.669238154446295\n",
      "iter 7900 loss 5.662478346092356\n",
      "iter 8000 loss 5.658508083923818\n",
      "iter 8100 loss 5.646739503115125\n",
      "iter 8200 loss 5.644762196374469\n",
      "iter 8300 loss 5.649516217438388\n",
      "iter 8400 loss 5.650464557131661\n",
      "iter 8500 loss 5.649302283361328\n",
      "iter 8600 loss 5.654129939186491\n",
      "iter 8700 loss 5.647665108069512\n",
      "iter 8800 loss 5.648043020552308\n",
      "iter 8900 loss 5.643510262124552\n",
      "iter 9000 loss 5.645641615206421\n",
      "iter 9100 loss 5.643412665002569\n",
      "iter 9200 loss 5.641276548704548\n",
      "iter 9300 loss 5.637701492064897\n",
      "iter 9400 loss 5.640435352764565\n",
      "iter 9500 loss 5.639869484631291\n",
      "iter 9600 loss 5.643242182455117\n",
      "iter 9700 loss 5.643397449012418\n",
      "iter 9800 loss 5.650681525529641\n",
      "iter 9900 loss 5.65167248826679\n",
      "iter 10000 loss 5.653317578612965\n",
      "iter 10100 loss 5.653593462028783\n",
      "iter 10200 loss 5.652070115914638\n",
      "iter 10300 loss 5.65300662312597\n",
      "iter 10400 loss 5.650215695447824\n",
      "iter 10500 loss 5.655197025758745\n",
      "iter 10600 loss 5.655841638834766\n",
      "iter 10700 loss 5.64895999324666\n",
      "iter 10800 loss 5.65044696273522\n",
      "iter 10900 loss 5.649549061640087\n",
      "iter 11000 loss 5.648328534704091\n",
      "iter 11100 loss 5.648978308982843\n",
      "iter 11200 loss 5.649786816461417\n",
      "iter 11300 loss 5.646894760795762\n",
      "iter 11400 loss 5.647935393389647\n",
      "iter 11500 loss 5.651292425535543\n",
      "epoch 2 loss 5.650016017188245\n",
      "iter 0 loss 10.701594352722168\n",
      "iter 100 loss 5.412630225467209\n",
      "iter 200 loss 5.316067261334083\n",
      "iter 300 loss 5.347623865172713\n",
      "iter 400 loss 5.442997345677635\n",
      "iter 500 loss 5.455046032419699\n",
      "iter 600 loss 5.431662635527514\n",
      "iter 700 loss 5.409289290323407\n",
      "iter 800 loss 5.37496136830243\n",
      "iter 900 loss 5.406163876315994\n",
      "iter 1000 loss 5.4033284570489615\n",
      "iter 1100 loss 5.388222492168861\n",
      "iter 1200 loss 5.368444035640863\n",
      "iter 1300 loss 5.339350886110339\n",
      "iter 1400 loss 5.364885762918854\n",
      "iter 1500 loss 5.3774023718789445\n",
      "iter 1600 loss 5.404054654456168\n",
      "iter 1700 loss 5.399927410621772\n",
      "iter 1800 loss 5.416979280182019\n",
      "iter 1900 loss 5.4561262398346795\n",
      "iter 2000 loss 5.473002847732752\n",
      "iter 2100 loss 5.4806702819851\n",
      "iter 2200 loss 5.463702927701098\n",
      "iter 2300 loss 5.447318765726985\n",
      "iter 2400 loss 5.447985445656214\n",
      "iter 2500 loss 5.448043901686285\n",
      "iter 2600 loss 5.435884196556151\n",
      "iter 2700 loss 5.430588671859659\n",
      "iter 2800 loss 5.4293167311904345\n",
      "iter 2900 loss 5.427453894577779\n",
      "iter 3000 loss 5.424130419827191\n",
      "iter 3100 loss 5.424697982437454\n",
      "iter 3200 loss 5.434160609662961\n",
      "iter 3300 loss 5.434214415544996\n",
      "iter 3400 loss 5.439205393922466\n",
      "iter 3500 loss 5.443314374970116\n",
      "iter 3600 loss 5.451215391501193\n",
      "iter 3700 loss 5.455209553881904\n",
      "iter 3800 loss 5.4660043109916945\n",
      "iter 3900 loss 5.4813669173228865\n",
      "iter 4000 loss 5.47869602710165\n",
      "iter 4100 loss 5.469868142429401\n",
      "iter 4200 loss 5.479329129158245\n",
      "iter 4300 loss 5.495349756969709\n",
      "iter 4400 loss 5.505007011965307\n",
      "iter 4500 loss 5.516929741347135\n",
      "iter 4600 loss 5.528253957117492\n",
      "iter 4700 loss 5.538624047475453\n",
      "iter 4800 loss 5.543355373911275\n",
      "iter 4900 loss 5.5415849527110375\n",
      "iter 5000 loss 5.543563848503278\n",
      "iter 5100 loss 5.552993557329179\n",
      "iter 5200 loss 5.560232607903125\n",
      "iter 5300 loss 5.562509881420712\n",
      "iter 5400 loss 5.576342985845124\n",
      "iter 5500 loss 5.583399617182756\n",
      "iter 5600 loss 5.580624626507463\n",
      "iter 5700 loss 5.581219387614315\n",
      "iter 5800 loss 5.5895039543363065\n",
      "iter 5900 loss 5.588137307440885\n",
      "iter 6000 loss 5.589576846879138\n",
      "iter 6100 loss 5.5830799877726625\n",
      "iter 6200 loss 5.578732474830973\n",
      "iter 6300 loss 5.581421110239261\n",
      "iter 6400 loss 5.58177235269757\n",
      "iter 6500 loss 5.599053130594424\n",
      "iter 6600 loss 5.59211879391875\n",
      "iter 6700 loss 5.590917212656498\n",
      "iter 6800 loss 5.593012479392856\n",
      "iter 6900 loss 5.588901459775599\n",
      "iter 7000 loss 5.584368476000042\n",
      "iter 7100 loss 5.584015807516977\n",
      "iter 7200 loss 5.5854052959270994\n",
      "iter 7300 loss 5.586319460152534\n",
      "iter 7400 loss 5.586704925119418\n",
      "iter 7500 loss 5.589659703777498\n",
      "iter 7600 loss 5.5918077095302055\n",
      "iter 7700 loss 5.592906454173392\n",
      "iter 7800 loss 5.591782421614784\n",
      "iter 7900 loss 5.585069410530258\n",
      "iter 8000 loss 5.58120576627045\n",
      "iter 8100 loss 5.569047923173276\n",
      "iter 8200 loss 5.566698814438489\n",
      "iter 8300 loss 5.5704428233837255\n",
      "iter 8400 loss 5.570300889407548\n",
      "iter 8500 loss 5.56858392733081\n",
      "iter 8600 loss 5.573254790150583\n",
      "iter 8700 loss 5.566020709197764\n",
      "iter 8800 loss 5.566098105000266\n",
      "iter 8900 loss 5.560980603698197\n",
      "iter 9000 loss 5.561824669171845\n",
      "iter 9100 loss 5.559672093991656\n",
      "iter 9200 loss 5.557237100744038\n",
      "iter 9300 loss 5.552690010401283\n",
      "iter 9400 loss 5.554513912084077\n",
      "iter 9500 loss 5.5537828361046415\n",
      "iter 9600 loss 5.557084156399145\n",
      "iter 9700 loss 5.556409081705464\n",
      "iter 9800 loss 5.562831723634387\n",
      "iter 9900 loss 5.563580640344388\n",
      "iter 10000 loss 5.564595931350333\n",
      "iter 10100 loss 5.564001697233709\n",
      "iter 10200 loss 5.562135768711199\n",
      "iter 10300 loss 5.562775507242387\n",
      "iter 10400 loss 5.5599832969790475\n",
      "iter 10500 loss 5.565011266644155\n",
      "iter 10600 loss 5.5653910505039015\n",
      "iter 10700 loss 5.558322835681249\n",
      "iter 10800 loss 5.558947176657257\n",
      "iter 10900 loss 5.5578232561844265\n",
      "iter 11000 loss 5.556088198773829\n",
      "iter 11100 loss 5.556340727999064\n",
      "iter 11200 loss 5.556644885683792\n",
      "iter 11300 loss 5.55372754667103\n",
      "iter 11400 loss 5.553984009184949\n",
      "iter 11500 loss 5.556600058913677\n",
      "epoch 3 loss 5.555316624824355\n"
     ]
    }
   ],
   "source": [
    "train_rnn(rnn, optimizer_rnn, criterion_rnn, TRAIN, RNN_NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "R5JxdK30Cv8u",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RNN---Test (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "L7yKgJmZ5iZ5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Even if loss went down, we can't make any guarantees about what the network will do on unseen sequences. To evaluate, we will measure **perplexity**, how much the network is confused by data. As you adjust hyperparameters and retrain the model you may notice that a model with a lower loss on training data doesn't necessarily produce a model with lower perplexity on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": false,
    "id": "Ztoctsl_lu1r",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  tensor(1427.0468)\n",
      "Test D: 20/20\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value < 2000 to receive full credit (no partial credit)\n",
    "ag.evaluate_rnn(rnn, TEST, criterion_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "fPbSy48UCzws",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RNN---Generate (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "g8oloDU5C2-S",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's use the RNN to generate some text. This is going to take a bit of set up. We need to take an input prompt---the start of the text---and tokenize it. Then we need a hidden state that represents the prompt, so we have to run the input prompt through the RNN to build up the hidden state. Then we can finally let the RNN loose to generate new text by feeding the outputs of the RNN (and the hidden state) back into the RNN as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": false,
    "id": "QMlveLOcm8A9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input prompt: the First War began\n",
      "input tokens: [2, 14, 15, 16] \n",
      "\n",
      "Prepping hidden state:\n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 5 of \n",
      "\n",
      "current token: 14 first\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 15 war\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 16 began\n",
      "predicted next token: 2 the \n",
      "\n",
      "Generating continuation:\n",
      "\n",
      "current token: 16 began\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 5 of \n",
      "\n",
      "current token: 5 of\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 5 of \n",
      "\n",
      "current token: 5 of\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 5 of \n",
      "\n",
      "current token: 5 of\n",
      "predicted next token: 2 the \n",
      "\n",
      "Final continuation:\n",
      "[2, 2, 5, 2, 2, 5, 2, 2, 5, 2]\n",
      "['the', 'the', 'of', 'the', 'the', 'of', 'the', 'the', 'of', 'the']\n",
      "Final:\n",
      "the First War began the the of the the of the the of the\n"
     ]
    }
   ],
   "source": [
    "# Example input prompt:\n",
    "input_prompt = \"the First War began\"\n",
    "# How long should the continuation be?\n",
    "num_new_tokens = 10\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = normalize_string(input_prompt)\n",
    "# Tokenize the input\n",
    "tokenized_input = [VOCAB.word2index(w) for w in normalized_input.split()]\n",
    "print(\"input prompt:\", input_prompt)\n",
    "print(\"input tokens:\", tokenized_input, '\\n')\n",
    "\n",
    "# We need to make a hidden_state that is representative of what is in the input prompt\n",
    "def prep_hidden_state(tokenized_input, rnn, verbose=False):\n",
    "  # Get an initial hidden state\n",
    "  hidden_state = rnn.init_hidden()\n",
    "  # Run the input prompt through the RNN to build up the hidden state.\n",
    "  # Discard the outputs (we are not trying to make predictions) until we get to the end\n",
    "  for token in tokenized_input:\n",
    "    if verbose:\n",
    "      print(\"current token:\", token, VOCAB.index2word(token))\n",
    "    # Get the one-hot for the current token\n",
    "    x = token2onehot(token)\n",
    "    x = x.float()\n",
    "    # Run the current one-hot and hidden state through the RNN\n",
    "    output, hidden_state = rnn(x, hidden_state)\n",
    "    # Get the highest predicted token\n",
    "    next_token = output.argmax().item()\n",
    "    if verbose:\n",
    "      print(\"predicted next token:\", next_token, VOCAB.index2word(next_token), '\\n')\n",
    "  return hidden_state\n",
    "\n",
    "# Get the hidden state that represents the input prompt\n",
    "print(\"Prepping hidden state:\\n\")\n",
    "hidden_state = prep_hidden_state(tokenized_input, rnn, verbose=True)\n",
    "\n",
    "# Generate a continuation by sampling from the RNN and then feeding the predicted output\n",
    "# back into the RNN over and over. The default sampling is argmax.\n",
    "def generate_rnn(rnn, num_new_tokens, token, hidden_state, fn=lambda d:d.argmax().item(), verbose=False):\n",
    "  # Keep generating more by feeding the predicted output back into the RNN as input\n",
    "  # Start with the last token of the input prompt and the newly prepped hidden state\n",
    "  if verbose:\n",
    "    print(\"Generating continuation:\\n\")\n",
    "  continuation = []\n",
    "  for n in range(num_new_tokens):\n",
    "    if verbose:\n",
    "      print(\"current token:\", token, VOCAB.index2word(token))\n",
    "    # Get the one-hot for the current token\n",
    "    x = token2onehot(token)\n",
    "    x = x.float()\n",
    "    # Run the current one-hot through the RNN\n",
    "    output, hidden_state = rnn(x, hidden_state)\n",
    "    # Predict the next token\n",
    "    next_token = fn(output)\n",
    "    if verbose:\n",
    "      print(\"predicted next token:\", next_token, VOCAB.index2word(next_token), '\\n')\n",
    "    # Remember the new token\n",
    "    continuation.append(next_token)\n",
    "    # update the current\n",
    "    token = next_token\n",
    "  return continuation\n",
    "\n",
    "# Generate the continuation. Use the argmax function to sample from the RNN's outputs\n",
    "token = tokenized_input[-1]\n",
    "continuation = generate_rnn(rnn, num_new_tokens, token, hidden_state, verbose=True)\n",
    "\n",
    "# All done\n",
    "print(\"Final continuation:\")\n",
    "print(continuation)\n",
    "continuation_text = [VOCAB.index2word(t) for t in continuation]\n",
    "print(continuation_text)\n",
    "print(\"Final:\")\n",
    "print(input_prompt + ' ' + ' '.join(continuation_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "D7AaCW1qwgT-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Odds are good that you got an output that was highly repetitive. This is in part because we always take the `argmax` of the output logits. There are sequences in the corpus that are highly probable, so by sampling the most likely logit, we are going to get trapped in a local max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2YCYDJPo4-gi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Instead, we need to treat the output of the RNN as a distribution and *sample* from the distribution to proabilisticially choose the next token, proportional to how highly activated each token is.\n",
    "\n",
    "**Complete the following function.** `my_sample()` should take a tensor of log probabilities for each token in the the vocabulary (the output of the RNN). It should probabilistically sample from this distribution and return a highly probable next token as an integer.\n",
    "\n",
    "**Hints:** Consider using `torch.multinomial`. Remember, your input is in log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true,
    "id": "Nu-_4kGv4Yv8",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69f4926e06aa0885",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def my_sample(log_probs):\n",
    "  token = None\n",
    "  ### BEGIN SOLUTION\n",
    "\n",
    "  # Given the hint, we need to convert log probability to probablity first\n",
    "  # Then using the probablity we sample using torch.multinomial\n",
    "  # print(log_probs.shape)  #(1, vocab)\n",
    "  probability = torch.exp(log_probs)\n",
    "  token = torch.multinomial(probability, num_samples=1)\n",
    "  token = token.item()\n",
    "\n",
    "  ### END SOLUTION\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": false,
    "id": "iop8mlR7gShH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  96\n",
      "Test E: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value > 90 to receive full credit (no partial credit)\n",
    "ag.unit_test_my_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "GA_n9Dr6Bmhp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below a few times and see what gets generated with your sampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": false,
    "id": "brCksz3sw4cr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input prompt: the First War began\n",
      "input tokens: [2, 14, 15, 16] \n",
      "\n",
      "Generating continuation:\n",
      "\n",
      "current token: 16 began\n",
      "predicted next token: 32 earth \n",
      "\n",
      "current token: 32 earth\n",
      "predicted next token: 309 under \n",
      "\n",
      "current token: 309 under\n",
      "predicted next token: 103 when \n",
      "\n",
      "current token: 103 when\n",
      "predicted next token: 39 . \n",
      "\n",
      "current token: 39 .\n",
      "predicted next token: 650 what \n",
      "\n",
      "current token: 650 what\n",
      "predicted next token: 760 teleri \n",
      "\n",
      "current token: 760 teleri\n",
      "predicted next token: 10 told \n",
      "\n",
      "current token: 10 told\n",
      "predicted next token: 277 him \n",
      "\n",
      "current token: 277 him\n",
      "predicted next token: 40 but \n",
      "\n",
      "current token: 40 but\n",
      "predicted next token: 258 desired \n",
      "\n",
      "Final continuation:\n",
      "[32, 309, 103, 39, 650, 760, 10, 277, 40, 258]\n",
      "earth under when . what teleri told him but desired\n",
      "Final:\n",
      "the First War began earth under when . what teleri told him but desired\n"
     ]
    }
   ],
   "source": [
    "# Example input prompt:\n",
    "input_prompt = \"the First War began\"\n",
    "# How long should the continuation be?\n",
    "num_new_tokens = 10\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = normalize_string(input_prompt)\n",
    "# Tokenize the input\n",
    "tokenized_input = [VOCAB.word2index(w) for w in normalized_input.split()]\n",
    "print(\"input prompt:\", input_prompt)\n",
    "print(\"input tokens:\", tokenized_input, '\\n')\n",
    "# Get an initial hidden state\n",
    "hidden_state = prep_hidden_state(tokenized_input, rnn)\n",
    "\n",
    "# Generate the continuation. Use my_sample\n",
    "token = tokenized_input[-1]\n",
    "continuation = generate_rnn(rnn, num_new_tokens, token, hidden_state, fn=my_sample, verbose=True)\n",
    "\n",
    "# All done\n",
    "print(\"Final continuation:\")\n",
    "print(continuation)\n",
    "continuation_text = [VOCAB.index2word(t) for t in continuation]\n",
    "print(' '.join(continuation_text))\n",
    "print(\"Final:\")\n",
    "print(input_prompt + ' ' + ' '.join(continuation_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "BUPq4bIjRV3i",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RNN---Optimize (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "k-yYCls275hX",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Well, that is probably crazy and non-sensical. If the distribution has a lot of nearly-equal probability candidates, the sampling technique you wrote probably makes some choices that look random. But it probably isn't stuck in a local max.\n",
    "\n",
    "How do we fix this? We introduce something called **temperature**. Temperature is a value between 0.0 and 1.0 that makes higher probability tokens more probable and less probable tokens less probable.\n",
    "\n",
    "**Complete the following function.** It should operate exactly like `my_sample()` except that it should divide the probabilities (between 0.0 and 1.0) by temperature.\n",
    "\n",
    "Dividing by `temperature=1.0` leaves the probability distribution unchanged. As temperature gets smaller, approaching 0.0, the high probability tokens approach infinity faster than low probability tokens. The distribution spreads out along an exponential curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true,
    "id": "AB-26jCc-hvX",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a18829f8109ede9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def my_temperature_sample(log_probs, temperature=1.0):\n",
    "  token = None\n",
    "  \n",
    "  # Given the hint, we need to convert log probability to probablity first\n",
    "  # Then using the probablity we sample using torch.multinomial\n",
    "  # print(log_probs.shape)  #(1, vocab)\n",
    "\n",
    "  # divide the log prob with temperature\n",
    "  log_probs = log_probs / temperature\n",
    "  # convert to probability\n",
    "  probability = torch.exp(log_probs)\n",
    "  token = torch.multinomial(probability, num_samples=1)\n",
    "  token = token.item()\n",
    "\n",
    "  \n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": false,
    "id": "spkbQciwhgM0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value:  0.0\n",
      "Test F: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return True to receive full credit (no partial credit)\n",
    "ag.unit_test_my_temperature_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uhnhTbWIBEl0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "One more time. This time, play around with the temperature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "id": "UmMpnUNxCOVM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# set the temperature - it's ok to modify this cell\n",
    "RNN_TEMPERATURE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": false,
    "id": "fXT4cW4BBJWV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input prompt: the First War began\n",
      "input tokens: [2, 14, 15, 16] \n",
      "\n",
      "Generating continuation:\n",
      "\n",
      "current token: 16 began\n",
      "predicted next token: 22 and \n",
      "\n",
      "current token: 22 and\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 105 were \n",
      "\n",
      "current token: 105 were\n",
      "predicted next token: 441 many \n",
      "\n",
      "current token: 441 many\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 41 in \n",
      "\n",
      "current token: 41 in\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 139 they \n",
      "\n",
      "current token: 139 they\n",
      "predicted next token: 5 of \n",
      "\n",
      "Final continuation:\n",
      "[22, 2, 105, 441, 2, 2, 41, 2, 139, 5]\n",
      "and the were many the the in the they of\n",
      "Final:\n",
      "the First War began and the were many the the in the they of\n"
     ]
    }
   ],
   "source": [
    "# Example input prompt:\n",
    "input_prompt = \"the First War began\"\n",
    "# How long should the continuation be?\n",
    "num_new_tokens = 10\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = normalize_string(input_prompt)\n",
    "# Tokenize the input\n",
    "tokenized_input = [VOCAB.word2index(w) for w in normalized_input.split()]\n",
    "print(\"input prompt:\", input_prompt)\n",
    "print(\"input tokens:\", tokenized_input, '\\n')\n",
    "# Get an initial hidden state\n",
    "hidden_state = prep_hidden_state(tokenized_input, rnn)\n",
    "\n",
    "# Generate the continuation. Use my_sample\n",
    "token = tokenized_input[-1]\n",
    "continuation = generate_rnn(rnn, num_new_tokens, token, hidden_state, fn=lambda d:my_temperature_sample(d, RNN_TEMPERATURE), verbose=True)\n",
    "\n",
    "# All done\n",
    "print(\"Final continuation:\")\n",
    "print(continuation)\n",
    "continuation_text = [VOCAB.index2word(t) for t in continuation]\n",
    "print(' '.join(continuation_text))\n",
    "print(\"Final:\")\n",
    "print(input_prompt + ' ' + ' '.join(continuation_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "qXU9cSZjRV3k",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Grading\n",
    "\n",
    "Please submit this .ipynb file to Canvas for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Final Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your projected points for this assignment is 70/70.\n",
      "\n",
      "NOTE: THIS IS NOT YOUR FINAL GRADE. YOUR FINAL GRADE FOR THIS ASSIGNMENT WILL BE AT LEAST 70 OR MORE, BUT NOT LESS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# student check\n",
    "ag.final_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Notebook Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook execution time in minutes = 10.087188673019408\n"
     ]
    }
   ],
   "source": [
    "# end time - notebook execution\n",
    "end_nb = time.time()\n",
    "# print notebook execution time in minutes\n",
    "print(\"Notebook execution time in minutes =\", (end_nb - start_nb)/60)\n",
    "# warn student if notebook execution time is greater than 30 minutes\n",
    "if (end_nb - start_nb)/60 > 30:\n",
    "  print(\"WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UT-whgM5YV8X",
    "VeQWnSY8CgT1",
    "UY0I2wl9D61t"
   ],
   "provenance": [
    {
     "file_id": "1vvRsIWcSStCzw7Z28SRnMkJfaENRnwb9",
     "timestamp": 1686165584566
    },
    {
     "file_id": "1Io6bUVyvhylwUuNavlc5pkVdCUq1fa13",
     "timestamp": 1682088867444
    }
   ]
  },
  "kernelspec": {
   "display_name": "cs7650_HW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
