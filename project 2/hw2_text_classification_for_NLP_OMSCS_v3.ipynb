{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ywvLShfXIKS1",
    "tags": []
   },
   "source": [
    "# NLP Classification\n",
    "\n",
    "In this assignment we look at several ways of classifying texts:\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Multinomial Regression\n",
    "\n",
    "We also look at binary label classification problems (e.g., sentiment analysis) and multinomial classification problems (e.g., topic analysis).\n",
    "\n",
    "We will use two datasets:\n",
    "- [IMDb movie review sentiment](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "- [AG News topics](https://huggingface.co/datasets/ag_news)\n",
    "\n",
    "**Tips:**\n",
    "- Read all the code. We don't ask you to write the training loops, or evaluation loops, but it is often instructive to see how the models are trained and evaluated.\n",
    "- If you have a model that is learning (loss is decreasing), but you want to increase accuracy, try using ``nn.Dropout`` layers just before the final linear layer to force the model to handle missing or unfamiliar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "id": "BIktLO-8681K",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# start time - notebook execution\n",
    "import time\n",
    "start_nb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "1xVv6McXodZG",
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "vMRiPTv7vHe2",
    "tags": []
   },
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "id": "S5VeRBOiIKS4",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "id": "tskEz4ZU681L",
    "tags": []
   },
   "source": [
    "# Initialize the Autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "id": "t82U7CdK681L",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# import the autograder tests\n",
    "import hw2_tests as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "qyRo0v0JIKTK",
    "tags": []
   },
   "source": [
    "# Functions for cleaning up raw texts and tokenizing the corpus\n",
    "\n",
    "We perform text preprocessing that includes: removing HTML tags, making text lower case, stemming, and disposing of stopwords.\n",
    "In the end, we will split the entire dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": false,
    "id": "JvfvVWHyIKTL",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= [ps.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "id": "Wr_LdWWoIKTL",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "stopwords_english = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "print(stopwords_english)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, stopword_list):\n",
    "    tokens = [token.strip() for token in text]\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": false,
    "id": "NCXHv_tZIKTM",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def tokenize_and_clean(line, stem_and_remove_stop_words = True):\n",
    "\n",
    "    line = re.sub(r\"<.*?>\", \"\", line).strip() # remove all HTML tags\n",
    "    line = re.sub(r'[^a-zA-Z0-9]', ' ', line) # remove punc\n",
    "    line = line.lower().split()  # lower case\n",
    "    if stem_and_remove_stop_words:\n",
    "        line = remove_stopwords(line, stopwords_english)\n",
    "        line = simple_stemmer(line)\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ohc6u_lDIKS-",
    "tags": []
   },
   "source": [
    "# Download and unpack the sentiment data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "zIklHox9vjMI",
    "tags": []
   },
   "source": [
    "We are using IMDb Dataset for binary sentiment classification that provides a set of 25K highly polar reviews for training, and 25K for testing\n",
    "(each set contains an equal number of positive and negative examples).\n",
    "\n",
    "Dataset folder structure is as follows:\n",
    "\n",
    "dataset/ \\\n",
    "├── test/ \\\n",
    "│     ├── pos/ \\\n",
    "│     ├── neg/ \\\n",
    "├── train/ \\\n",
    "      ├── pos/ \\\n",
    "      └── neg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "id": "pxYJtzVj681M",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# check if dataset is downloaded\n",
    "if not os.path.isfile('aclImdb_v1.tar'):\n",
    "    print(\"Downloading dataset...\")\n",
    "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !gunzip aclImdb_v1.tar.gz\n",
    "    !tar -xvf aclImdb_v1.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "GRD03KapvnI0",
    "tags": []
   },
   "source": [
    "Load in the text from the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": false,
    "id": "HI3VdrTnIKTO",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def load_text_from_folders(path, file_list, dataset, samples = 2000, stem_and_remove_stop_words = True):\n",
    "    \"\"\"Read set of files from given directory and save returned lines to list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Absolute or relative path to given file (or set of files).\n",
    "    file_list: list\n",
    "        List of files names to read.\n",
    "    dataset: list\n",
    "        List that stores read lines.\n",
    "    samples: int\n",
    "        Number of samples in the output\n",
    "    \"\"\"\n",
    "    for i, file in enumerate(file_list):\n",
    "        if i >= samples:\n",
    "            break\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf8') as text:\n",
    "            contents = text.read()\n",
    "            contents_tokenized = tokenize_and_clean(contents, stem_and_remove_stop_words=stem_and_remove_stop_words)\n",
    "            dataset.append(contents_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "_8KuCNoXIKTO",
    "tags": []
   },
   "source": [
    "# Creating training and test sets\n",
    "\n",
    "This creates four arrays:\n",
    "\n",
    "\n",
    "*   ```train_pos``` -- instances in the training set with positive sentiment labels\n",
    "*   ```train_neg``` -- instances in the training set with negative sentiment labels\n",
    "*   ```test_pos``` -- instances in the testing set with positive sentiment labels\n",
    "*   ```test_neg``` -- instances in the testing set with negative sentiment labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "id": "4Nv5FvBIIKTO",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Path to dataset location\n",
    "path = 'aclImdb/'\n",
    "\n",
    "# Create lists that will contain read lines\n",
    "train_pos, train_neg, test_pos, test_neg = [], [], [], []\n",
    "\n",
    "# Create a dictionary of paths and lists that store lines (key: value = path: list)\n",
    "sets_dict = {'train/pos/': train_pos, 'train/neg/': train_neg,\n",
    "             'test/pos/': test_pos, 'test/neg/': test_neg}\n",
    "\n",
    "# Load the data\n",
    "for dataset in sets_dict:\n",
    "  file_list = [f for f in sorted(os.listdir(os.path.join(path, dataset))) if f.endswith('.txt')]\n",
    "  load_text_from_folders(os.path.join(path, dataset), file_list, sets_dict[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "3t5nCE_xpvx7",
    "tags": []
   },
   "source": [
    "Convert into Pandas dataframes. Pandas is a virtual spreadsheet with a programmatic API. A ```DataFrame``` is a spreadsheet. We will make a spreadsheet of training data and one for testing data and one with everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": false,
    "id": "kovNgXqIIKTP",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Concatenate training and testing examples into one dataset\n",
    "TRAIN = pd.concat([pd.DataFrame({'review': train_pos, 'label':1}),\n",
    "                     pd.DataFrame({'review': train_neg, 'label':0})],\n",
    "                     axis=0, ignore_index=True)\n",
    "\n",
    "TEST = pd.concat([pd.DataFrame({'review': test_pos, 'label':1}),\n",
    "                    pd.DataFrame({'review': test_neg, 'label':0})],\n",
    "                    axis=0, ignore_index=True)\n",
    "\n",
    "ALL = pd.concat([TRAIN, TEST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "CKUFtqERp79N",
    "tags": []
   },
   "source": [
    "Look at the data.\n",
    "\n",
    "This is a summary of the data. We see that the data is balanced between labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "id": "0xPnt_CDIKTP",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    2000\n",
       "0    2000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "QlV_TMBLqOYB",
    "tags": []
   },
   "source": [
    "This is the first few rows of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": false,
    "id": "InVC2IvnIKTQ",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[homeless, houseless, georg, carlin, state, is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[brilliant, act, lesley, ann, warren, best, dr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[easili, underr, film, inn, brook, cannon, sur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  [bromwel, high, cartoon, comedi, ran, time, pr...      1\n",
       "1  [bromwel, high, cartoon, comedi, ran, time, pr...      1\n",
       "2  [homeless, houseless, georg, carlin, state, is...      1\n",
       "3  [brilliant, act, lesley, ann, warren, best, dr...      1\n",
       "4  [easili, underr, film, inn, brook, cannon, sur...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "3jP5U6usIKTR",
    "tags": []
   },
   "source": [
    "# Creating a vocabulary file\n",
    "\n",
    "Next, we have to build a vocabulary. This is effectively a look-up table where every unique word in your data set has a corresponding index (an integer).\n",
    "We do this as our machine learning model cannot operate on strings, but only numbers. Each index is used to construct a one-hot vector for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": false,
    "id": "Oz6hQlCgqckU",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._word2index = {}\n",
    "        self._word2count = {}\n",
    "        self._index2word = {}\n",
    "        self._n_words = 0\n",
    "\n",
    "    def get_words(self):\n",
    "      return list(self._word2count.keys())\n",
    "\n",
    "    def num_words(self):\n",
    "      return self._n_words\n",
    "\n",
    "    def word2index(self, word):\n",
    "      return self._word2index[word]\n",
    "\n",
    "    def index2word(self, word):\n",
    "      return self._index2word[word]\n",
    "\n",
    "    def word2count(self, word):\n",
    "      return self._word2count[word]\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self._word2index:\n",
    "            self._word2index[word] = self._n_words\n",
    "            self._word2count[word] = 1\n",
    "            self._index2word[self._n_words] = word\n",
    "            self._n_words += 1\n",
    "        else:\n",
    "            self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UT7oI7pXw9wA",
    "tags": []
   },
   "source": [
    "Make a vocab object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "id": "-FLxtGNgw79Q",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "VOCAB = Vocab(\"imdb\")\n",
    "VOCAB_SIZE = 1000\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ggqgPtCSxAG-",
    "tags": []
   },
   "source": [
    "Load the first ```n``` frequent words in the vocabulary. Do this by sorting by frequency and then truncating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": false,
    "id": "DAiKNELzwC2T",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Get word frequency counts\n",
    "word_freq_dict = {}   # key = word, value = frequency\n",
    "for review in ALL['review']:\n",
    "  for word in review:\n",
    "    if word in word_freq_dict:\n",
    "      word_freq_dict[word] += 1\n",
    "    else:\n",
    "      word_freq_dict[word] = 1\n",
    "\n",
    "# Get a list of (word, freq) tuples sorted by frequency\n",
    "kv_list = []  # list of word-freq tuples so can sort\n",
    "for (k,v) in word_freq_dict.items():\n",
    "  kv_list.append((k,v))\n",
    "sorted_kv_list = sorted(kv_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Load top n words in to vocab object\n",
    "for word, freq in sorted_kv_list[:VOCAB_SIZE]:\n",
    "  VOCAB.add_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "hvh0yHT_IKTi",
    "tags": []
   },
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes Algorithm is based on the Bayes Rule which describes the probability of an event,\n",
    "based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "According to Bayes theorem:\n",
    "\n",
    "\n",
    "```Posterior = likelihood * proposition/evidence```\n",
    "\n",
    "or\n",
    "\n",
    "```P(A|B) = P(B|A) * P(A)/P(B)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "vELAfeWDRrNq",
    "tags": []
   },
   "source": [
    "Using word presence as features, create an array of features for each review. Each review will thus be an array of size ```len(vocab)``` where each index in the array is a token number and the value in that position is whether the token is present in the review. There will be ```num_rows``` arrays, making a ```num_rows x len(vocab)``` 2D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "DFf6zYmozBtc",
    "tags": []
   },
   "source": [
    "This function creates a bag of words. It returns a vector where each element is a count of the words in the sentence corresponding to the word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": false,
    "id": "DKCwfenwIKTw",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def make_bow(sentence):\n",
    "    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64)\n",
    "    for word in sentence:\n",
    "        if word not in VOCAB.get_words():\n",
    "            continue\n",
    "        vec[VOCAB.word2index(word)] += 1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "NHweI8gPzOdh",
    "tags": []
   },
   "source": [
    "Prepare data ```X_TRAIN``` is a 2D array of size ```num_reviews x vocab_size``` that contains training data. Each row will be a bag of words, except each index contains a 1 or 0 based on word presence in the example. Each row is a vector of features $\\phi_1 ... \\phi_{|V|}$ assumed to be independent, where $|V|$ is size of the vocabulary. We don't need to know what the features are, only whether they are present in each example in the training set.\n",
    "\n",
    "```X_TEST``` is the same as above but containing testing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": false,
    "id": "c30F5ZwyIKTw",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Vectorize text reviews to numbers\n",
    "# Make empty vectors\n",
    "X_TRAIN = np.zeros((len(TRAIN), VOCAB_SIZE))\n",
    "X_TEST = np.zeros((len(TEST), VOCAB_SIZE))\n",
    "\n",
    "# Load in frequency counts\n",
    "for i, row in TRAIN.iterrows():\n",
    "    X_TRAIN[i] = np.array(make_bow(row['review'])) > 0 # The > 0 converts to presence instead of counts\n",
    "\n",
    "for i, row in TEST.iterrows():\n",
    "    X_TEST[i] = np.array(make_bow(row['review'])) > 0 # The > 0 converts to presence instead of counts\n",
    "\n",
    "# The labels\n",
    "Y_TRAIN = np.array(TRAIN['label'])\n",
    "Y_TEST = np.array(TEST['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ao16p7_U7Zdz",
    "tags": []
   },
   "source": [
    "What you want to do is to compute probabilities over the training data and then apply those probabilities to the testing examples. Use the Bayes formula to compute $P_{\\rm test}(L_{+}|\\phi_{0:|V|})$ and $P_{\\rm test}(L_{-}|\\phi_{0:|V|})$ for each review. Classify examples based on whether one probability is higher than another. That is, $sign(P_{\\rm test}(L_{+}|\\phi_{0:|V|}) - P_{\\rm test}(L_{-}|\\phi_{0:|V|}))$ indicates a positive review when greater than 0 and a negative review when less than 0.\n",
    "\n",
    "**Hint:** You do not need to implement any loops. Numpy indexing and slicing operations, along with built in functions like `.mean()`, `.sum()`, etc. will allow all operations to be performed on each row of the data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "Q2ch6U7uNmeN",
    "tags": []
   },
   "source": [
    "Step 1: Compute the positive label condition:\n",
    "$P(L_{+}|\\phi_{0:|V|}) = P(\\phi_{0:|V|}|L_{+})P(L_{+}) / P(\\phi_{0:|V|})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true,
    "id": "AwAAV9Zq7YbI",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e927fe9f8caf96a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prob_pos_given_features(x_train, y_train):\n",
    "  log_probs = np.array([0] * x_train.shape[1])\n",
    "  # print(log_probs.shape)\n",
    "\n",
    "  # Calculate P(L+)\n",
    "  P_L_pos = (y_train == 1).mean()\n",
    "\n",
    "  # Calculate P(Theta). Omit because we subtract in log scale.\n",
    "  # The 'evidence' probability is the same for both positive and negative label and cancel out during subtraction of log scale\n",
    "\n",
    "  # Select positive sample\n",
    "  x_train = x_train[y_train == 1]\n",
    "  # Count numbers of positive with each words present\n",
    "  words_counts = (x_train > 0).sum(axis=0)\n",
    "  # print(words_counts.shape)\n",
    "  # Apply smoothing\n",
    "  words_counts = words_counts + 1\n",
    "  likelihood = words_counts / words_counts.sum()\n",
    "  # print(likelihood.shape)\n",
    "  # print(likelihood)\n",
    "\n",
    "  log_probs = np.log(likelihood) + np.log(P_L_pos)\n",
    "\n",
    "  return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "5eeKepgkNpNa",
    "tags": []
   },
   "source": [
    "Step 2: Compute the negative label condition:\n",
    "$P(L_{-}|\\phi_{0:|V|}) = P(\\phi_{0:|V|}|L_{-})P(L_{-}) / P(\\phi_{0:|V|})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true,
    "id": "nWP9U3BuMess",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-996c909da2d8b8bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prob_neg_given_features(x_train, y_train):\n",
    "  log_probs = np.array([0] * x_train.shape[1])\n",
    "  # print(log_probs.shape)\n",
    "\n",
    "  # Calculate P(L+)\n",
    "  P_L_neg = (y_train == 0).mean()\n",
    "\n",
    "  # Calculate P(Theta). Omit because we subtract in log scale.\n",
    "  # The 'evidence' probability is the same for both positive and negative label and cancel out during subtraction of log scale\n",
    "\n",
    "  # Select positive sample\n",
    "  x_train = x_train[y_train == 0]\n",
    "  # Count numbers of positive with each words present\n",
    "  words_counts = (x_train > 0).sum(axis=0)\n",
    "  # print(words_counts.shape)\n",
    "  # Apply smoothing\n",
    "  words_counts = words_counts + 1\n",
    "  likelihood = words_counts / words_counts.sum()\n",
    "  # print(likelihood.shape)\n",
    "  # print(likelihood)\n",
    "\n",
    "  log_probs = np.log(likelihood) + np.log(P_L_neg)\n",
    "\n",
    "  return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": false,
    "id": "ZXdgx0Ku9sYq",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "pos_probs = prob_pos_given_features(X_TRAIN, Y_TRAIN)\n",
    "neg_probs = prob_neg_given_features(X_TRAIN, Y_TRAIN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "q9wbtxDZNuj3",
    "tags": []
   },
   "source": [
    "Step 3: Make a label prediction. Subtract (in log scale) the positive from the negative. If the result is greater than zero then it is a prediction of `+` label. If the result is less thn zero then we make a prediction of `-` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true,
    "id": "xaz7KBdd_CD1",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b4cde57fd68ac07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def naive_bayes(x, pos_probs, neg_probs):\n",
    "  label = 0\n",
    "  # print(x.shape)\n",
    "  # print(pos_probs.shape)\n",
    "  # print(neg_probs.shape)\n",
    "  # Calculate log-likelihood if positive label\n",
    "  log_likelihood_pos = np.sum(x * pos_probs)\n",
    "    \n",
    "  # Calculate log-likelihood if negative label\n",
    "  log_likelihood_neg = np.sum(x * neg_probs)\n",
    "    \n",
    "  # Compare\n",
    "  if log_likelihood_pos - log_likelihood_neg >= 0:\n",
    "    label = 1\n",
    "  else:\n",
    "    label = 0\n",
    "\n",
    "  return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VUOFzFVU_is3",
    "tags": []
   },
   "source": [
    "# Naive Bayes Test (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": false,
    "id": "T1rcPgnWP_6V",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.862\n",
      "Test A: 20/20\n"
     ]
    }
   ],
   "source": [
    "# student check - accuracies >= 78% will receive full credit (no credit for less than 78%)\n",
    "ag.test_naive_bayes(X_TRAIN, Y_TRAIN, X_TEST, Y_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "hUTymITZIKT5",
    "tags": []
   },
   "source": [
    "# Logistic Regression - Part 1\n",
    "\n",
    "We will be using a neural network to perform logistic regression. We will use word counts as the input feature vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "DWKJNb2CXhe5",
    "tags": []
   },
   "source": [
    "Reload the data, but use word counts instead of word presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": false,
    "id": "sBjsMbDSIKT6",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Randomize the data\n",
    "TRAIN = TRAIN.sample(frac=1).reset_index(drop=True)\n",
    "TEST = TEST.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Vectorize text reviews to numbers\n",
    "X_TRAIN = np.zeros((len(TRAIN), VOCAB_SIZE))\n",
    "X_TEST = np.zeros((len(TEST), VOCAB_SIZE))\n",
    "\n",
    "for i, row in TRAIN.iterrows():\n",
    "  X_TRAIN[i] = np.array(make_bow(row['review']))\n",
    "\n",
    "for i, row in TEST.iterrows():\n",
    "  X_TEST[i] = np.array(make_bow(row['review']))\n",
    "\n",
    "Y_TRAIN = np.array(TRAIN['label'])\n",
    "Y_TEST = np.array(TEST['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "mSBC93Ge6F1e",
    "tags": []
   },
   "source": [
    "Make a logistic classifier torch neural network.\n",
    "\n",
    "Complete the constructor and forward function. The net will take an arbitrary number of outputs, but for binary logistic regression, only one is needed because the single output neuron can take a value that is between 0 and 1, with 0 meaning negative sentiment and 1 meaning positive sentiment. There should only be as many parameters as ```num_features x (num_labels-1)``` in binary logistic regression and ```num_features x num_labels``` for multinomial logistic regression.\n",
    "\n",
    "The input will be a one-hot vector of size `vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "editable": true,
    "id": "yeJnVl7NIKT8",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6124df74ac9d8f1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Defining neural network structure\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "  def __init__(self, num_labels, vocab_size):\n",
    "    super(BoWClassifier, self).__init__()\n",
    "\n",
    "    # liniear layer\n",
    "    self.fc_layer = nn.Linear(vocab_size, num_labels)\n",
    "    # activation layer\n",
    "    # self.activation_layer = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, bow_vec):\n",
    "    # forward data through all layers\n",
    "    out = self.fc_layer(bow_vec)\n",
    "    out = torch.sigmoid(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": false,
    "id": "mgPYvkocIKT8",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# Use one label because the head can signify a 1 or 0 because of the sigmoid.\n",
    "bow_nn_model = BoWClassifier(NUM_LABELS-1, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "xaBdfHfDQ5oI",
    "tags": []
   },
   "source": [
    "This function should return two tensors. The first, containing training data, shoud be of size ```batch_size x vocab_size``` for the ```i```th batch. The second should be a list of labels of size ```batch_size```. Both tensors should be of type ```dtype=torch.float```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true,
    "id": "RPxjUwUTPZTq",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abf634c3a00755c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def get_batch(i, batch_size, x_data, y_data):\n",
    "  # Make some empty tensors\n",
    "  x = None\n",
    "  y = None\n",
    "  \n",
    "  # Set batch start and end\n",
    "  start_idx = i * batch_size\n",
    "  end_idx = start_idx + batch_size\n",
    "\n",
    "  # Slice data into batch using index\n",
    "  x = x_data[start_idx:end_idx]\n",
    "  y = y_data[start_idx:end_idx]\n",
    "\n",
    "  # Convert to tensor\n",
    "  x = torch.tensor(x, dtype=torch.float)\n",
    "  y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2eMh98LwOwv_",
    "tags": []
   },
   "source": [
    "# Logistic Regression - Part 1 Test (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": false,
    "id": "tp-B0qeXNnyo",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape looks good!\n",
      "Test B: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check\n",
    "ag.test_batch_output_shape(get_batch, X_TRAIN, Y_TRAIN, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "editable": false,
    "id": "FdPVSLtrN_2k",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has the expected number of layers.\n",
      "Test C: 5/5\n",
      "First layer is a Linear layer.\n",
      "Test D: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - your model must have the expected number of layers to receive full credit, no credit otherwise\n",
    "ag.check_bow_architecture(bow_nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "editable": false,
    "id": "FOYrmRVUj7g7",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output shape looks good!\n",
      "Test E: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check\n",
    "ag.test_forward_pass_shape(X_TRAIN, Y_TRAIN, bow_nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "rN5v9cjKif_H",
    "tags": []
   },
   "source": [
    "# Logistic Regression - Part 2\n",
    "\n",
    "Create a dataset as an array of (X_train, label).\n",
    "\n",
    "Complete ```get_batch(i)``` and set ```batch_size``` and ```num_epochs```.\n",
    "\n",
    "Training loop will call ```get_batch()``` with the iteration number and do everything else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": false,
    "id": "paJEFuigIKT-",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, train_data, test_data, epochs, batch_size):\n",
    "  n_iter = len(train_data) // batch_size\n",
    "  print(n_iter, 'batches per epoch')\n",
    "  # Loss Function\n",
    "  loss_function = nn.BCELoss()\n",
    "  # Optimizer initlialization\n",
    "  optimizer = optim.SGD(bow_nn_model.parameters(), lr=0.1)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # Make BOW vector for input features and target label\n",
    "    for i in range(n_iter):\n",
    "      x, y = get_batch(i, batch_size, train_data, test_data)\n",
    "\n",
    "      # Step 3. Run the forward pass.\n",
    "      y_hat = model(x)\n",
    "      y_hat = y_hat.reshape(-1)\n",
    "\n",
    "      # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "      loss = loss_function(y_hat,y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if (epoch+1)%10 == 0 and (i+1) == n_iter:\n",
    "        print('epoch:', epoch+1,',loss =',loss.item(), ', training accuracy =',(torch.round(y_hat)==y).float().mean())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "editable": true,
    "id": "gzD90s3KQgJd",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# It's ok to modify this cell.\n",
    "BATCH_SIZE = 100\n",
    "N_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "editable": false,
    "id": "Tv8Lqg8T949T",
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 batches per epoch\n",
      "epoch: 10 ,loss = 0.3434087634086609 , training accuracy = tensor(0.8700)\n",
      "epoch: 20 ,loss = 0.286634624004364 , training accuracy = tensor(0.8900)\n",
      "epoch: 30 ,loss = 0.25169429183006287 , training accuracy = tensor(0.9000)\n",
      "epoch: 40 ,loss = 0.2271960824728012 , training accuracy = tensor(0.9100)\n",
      "epoch: 50 ,loss = 0.20923738181591034 , training accuracy = tensor(0.9100)\n",
      "epoch: 60 ,loss = 0.1955214887857437 , training accuracy = tensor(0.9100)\n",
      "epoch: 70 ,loss = 0.18455684185028076 , training accuracy = tensor(0.9200)\n",
      "epoch: 80 ,loss = 0.17544421553611755 , training accuracy = tensor(0.9400)\n",
      "epoch: 90 ,loss = 0.1676492542028427 , training accuracy = tensor(0.9400)\n",
      "epoch: 100 ,loss = 0.1608404964208603 , training accuracy = tensor(0.9400)\n",
      "epoch: 110 ,loss = 0.15480048954486847 , training accuracy = tensor(0.9400)\n",
      "epoch: 120 ,loss = 0.14937911927700043 , training accuracy = tensor(0.9500)\n",
      "epoch: 130 ,loss = 0.14446790516376495 , training accuracy = tensor(0.9500)\n",
      "epoch: 140 ,loss = 0.13998568058013916 , training accuracy = tensor(0.9500)\n",
      "epoch: 150 ,loss = 0.13586974143981934 , training accuracy = tensor(0.9500)\n",
      "epoch: 160 ,loss = 0.1320704072713852 , training accuracy = tensor(0.9600)\n",
      "epoch: 170 ,loss = 0.12854748964309692 , training accuracy = tensor(0.9600)\n",
      "epoch: 180 ,loss = 0.12526792287826538 , training accuracy = tensor(0.9600)\n",
      "epoch: 190 ,loss = 0.12220417708158493 , training accuracy = tensor(0.9700)\n",
      "epoch: 200 ,loss = 0.1193326860666275 , training accuracy = tensor(0.9700)\n",
      "epoch: 210 ,loss = 0.11663386225700378 , training accuracy = tensor(0.9700)\n",
      "epoch: 220 ,loss = 0.11409059166908264 , training accuracy = tensor(0.9700)\n",
      "epoch: 230 ,loss = 0.11168818175792694 , training accuracy = tensor(0.9700)\n",
      "epoch: 240 ,loss = 0.10941379517316818 , training accuracy = tensor(0.9700)\n",
      "epoch: 250 ,loss = 0.10725636780261993 , training accuracy = tensor(0.9700)\n",
      "epoch: 260 ,loss = 0.10520600527524948 , training accuracy = tensor(0.9700)\n",
      "epoch: 270 ,loss = 0.10325397551059723 , training accuracy = tensor(0.9700)\n",
      "epoch: 280 ,loss = 0.10139256715774536 , training accuracy = tensor(0.9700)\n",
      "epoch: 290 ,loss = 0.09961507469415665 , training accuracy = tensor(0.9700)\n",
      "epoch: 300 ,loss = 0.09791523218154907 , training accuracy = tensor(0.9700)\n",
      "epoch: 310 ,loss = 0.09628757834434509 , training accuracy = tensor(0.9700)\n",
      "epoch: 320 ,loss = 0.0947270616889 , training accuracy = tensor(0.9700)\n",
      "epoch: 330 ,loss = 0.09322916716337204 , training accuracy = tensor(0.9700)\n",
      "epoch: 340 ,loss = 0.0917898491024971 , training accuracy = tensor(0.9700)\n",
      "epoch: 350 ,loss = 0.09040531516075134 , training accuracy = tensor(0.9700)\n",
      "epoch: 360 ,loss = 0.08907213062047958 , training accuracy = tensor(0.9700)\n",
      "epoch: 370 ,loss = 0.08778729289770126 , training accuracy = tensor(0.9700)\n",
      "epoch: 380 ,loss = 0.08654782921075821 , training accuracy = tensor(0.9700)\n",
      "epoch: 390 ,loss = 0.0853511318564415 , training accuracy = tensor(0.9700)\n",
      "epoch: 400 ,loss = 0.08419475704431534 , training accuracy = tensor(0.9700)\n",
      "epoch: 410 ,loss = 0.08307651430368423 , training accuracy = tensor(0.9700)\n",
      "epoch: 420 ,loss = 0.08199426531791687 , training accuracy = tensor(0.9700)\n",
      "epoch: 430 ,loss = 0.08094610273838043 , training accuracy = tensor(0.9700)\n",
      "epoch: 440 ,loss = 0.07993027567863464 , training accuracy = tensor(0.9700)\n",
      "epoch: 450 ,loss = 0.07894501090049744 , training accuracy = tensor(0.9800)\n",
      "epoch: 460 ,loss = 0.07798893004655838 , training accuracy = tensor(0.9800)\n",
      "epoch: 470 ,loss = 0.07706055045127869 , training accuracy = tensor(0.9800)\n",
      "epoch: 480 ,loss = 0.07615851610898972 , training accuracy = tensor(0.9800)\n",
      "epoch: 490 ,loss = 0.07528159022331238 , training accuracy = tensor(0.9800)\n",
      "epoch: 500 ,loss = 0.07442861795425415 , training accuracy = tensor(0.9800)\n",
      "epoch: 510 ,loss = 0.07359844446182251 , training accuracy = tensor(0.9800)\n",
      "epoch: 520 ,loss = 0.07279019802808762 , training accuracy = tensor(0.9800)\n",
      "epoch: 530 ,loss = 0.07200277596712112 , training accuracy = tensor(0.9800)\n",
      "epoch: 540 ,loss = 0.07123538851737976 , training accuracy = tensor(0.9800)\n",
      "epoch: 550 ,loss = 0.07048720121383667 , training accuracy = tensor(0.9800)\n",
      "epoch: 560 ,loss = 0.06975733488798141 , training accuracy = tensor(0.9900)\n",
      "epoch: 570 ,loss = 0.06904508918523788 , training accuracy = tensor(0.9900)\n",
      "epoch: 580 ,loss = 0.06834973394870758 , training accuracy = tensor(0.9900)\n",
      "epoch: 590 ,loss = 0.06767064332962036 , training accuracy = tensor(0.9900)\n",
      "epoch: 600 ,loss = 0.06700722873210907 , training accuracy = tensor(0.9900)\n",
      "epoch: 610 ,loss = 0.06635883450508118 , training accuracy = tensor(0.9900)\n",
      "epoch: 620 ,loss = 0.0657249316573143 , training accuracy = tensor(0.9900)\n",
      "epoch: 630 ,loss = 0.06510495394468307 , training accuracy = tensor(0.9900)\n",
      "epoch: 640 ,loss = 0.06449849903583527 , training accuracy = tensor(0.9900)\n",
      "epoch: 650 ,loss = 0.06390495598316193 , training accuracy = tensor(0.9900)\n",
      "epoch: 660 ,loss = 0.06332392245531082 , training accuracy = tensor(1.)\n",
      "epoch: 670 ,loss = 0.0627550333738327 , training accuracy = tensor(1.)\n",
      "epoch: 680 ,loss = 0.06219780072569847 , training accuracy = tensor(1.)\n",
      "epoch: 690 ,loss = 0.06165192648768425 , training accuracy = tensor(1.)\n",
      "epoch: 700 ,loss = 0.06111697852611542 , training accuracy = tensor(1.)\n",
      "epoch: 710 ,loss = 0.06059258431196213 , training accuracy = tensor(1.)\n",
      "epoch: 720 ,loss = 0.060078512877225876 , training accuracy = tensor(1.)\n",
      "epoch: 730 ,loss = 0.05957436189055443 , training accuracy = tensor(1.)\n",
      "epoch: 740 ,loss = 0.05907983332872391 , training accuracy = tensor(1.)\n",
      "epoch: 750 ,loss = 0.05859461426734924 , training accuracy = tensor(1.)\n",
      "epoch: 760 ,loss = 0.058118484914302826 , training accuracy = tensor(1.)\n",
      "epoch: 770 ,loss = 0.05765114724636078 , training accuracy = tensor(1.)\n",
      "epoch: 780 ,loss = 0.05719241127371788 , training accuracy = tensor(1.)\n",
      "epoch: 790 ,loss = 0.05674193426966667 , training accuracy = tensor(1.)\n",
      "epoch: 800 ,loss = 0.05629954859614372 , training accuracy = tensor(1.)\n",
      "epoch: 810 ,loss = 0.055864959955215454 , training accuracy = tensor(1.)\n",
      "epoch: 820 ,loss = 0.05543806403875351 , training accuracy = tensor(1.)\n",
      "epoch: 830 ,loss = 0.055018533021211624 , training accuracy = tensor(1.)\n",
      "epoch: 840 ,loss = 0.05460621789097786 , training accuracy = tensor(1.)\n",
      "epoch: 850 ,loss = 0.054200898855924606 , training accuracy = tensor(1.)\n",
      "epoch: 860 ,loss = 0.053802475333213806 , training accuracy = tensor(1.)\n",
      "epoch: 870 ,loss = 0.05341063439846039 , training accuracy = tensor(1.)\n",
      "epoch: 880 ,loss = 0.05302532762289047 , training accuracy = tensor(1.)\n",
      "epoch: 890 ,loss = 0.05264639854431152 , training accuracy = tensor(1.)\n",
      "epoch: 900 ,loss = 0.05227366462349892 , training accuracy = tensor(1.)\n",
      "epoch: 910 ,loss = 0.05190699175000191 , training accuracy = tensor(1.)\n",
      "epoch: 920 ,loss = 0.051546040922403336 , training accuracy = tensor(1.)\n",
      "epoch: 930 ,loss = 0.05119086429476738 , training accuracy = tensor(1.)\n",
      "epoch: 940 ,loss = 0.050841301679611206 , training accuracy = tensor(1.)\n",
      "epoch: 950 ,loss = 0.05049717426300049 , training accuracy = tensor(1.)\n",
      "epoch: 960 ,loss = 0.05015832930803299 , training accuracy = tensor(1.)\n",
      "epoch: 970 ,loss = 0.04982469230890274 , training accuracy = tensor(1.)\n",
      "epoch: 980 ,loss = 0.0494961179792881 , training accuracy = tensor(1.)\n",
      "epoch: 990 ,loss = 0.04917248710989952 , training accuracy = tensor(1.)\n",
      "epoch: 1000 ,loss = 0.04885370284318924 , training accuracy = tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bow_nn_model = train(bow_nn_model, X_TRAIN, Y_TRAIN, N_EPOCHS, BATCH_SIZE)\n",
    "except:\n",
    "    print(\"Training failed. Please check your code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "G4D7NpQlkBGf",
    "tags": []
   },
   "source": [
    "# Logistic Regression - Part 2 Test (20 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "editable": false,
    "id": "qW0uESVguCuL",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81      2000\n",
      "           1       0.81      0.80      0.80      2000\n",
      "\n",
      "    accuracy                           0.80      4000\n",
      "   macro avg       0.80      0.80      0.80      4000\n",
      "weighted avg       0.80      0.80      0.80      4000\n",
      "\n",
      "Accuracy:  0.8\n",
      "Test F: 20/20\n"
     ]
    }
   ],
   "source": [
    "# student check - accuracies >= 78% will receive full credit (no credit for less than 78%)\n",
    "ag.test_model_accuracy_lr(TEST, bow_nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "DphmlDCk5sr5",
    "tags": []
   },
   "source": [
    "# Multinomial Regression\n",
    "\n",
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "editable": false,
    "id": "SIiy4PBJVh7h",
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (0.31.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from aiohttp->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->datasets) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\trung\\miniconda3\\envs\\cs7650_hw\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "editable": false,
    "id": "GrcwUiN-VfSf",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "gqcveFex52Fy",
    "tags": []
   },
   "source": [
    "Unlike earlier, we will use a pre-defined set of embeddings, called [GLoVe](https://nlp.stanford.edu/projects/glove/). GLoVe replaces every word with a 100-dimensional vector of floating point values. The advantage of this is that words with similar semantic meanings will have similar vectors. This is important because the vocabulary size of the corpus we will use is 400,000.\n",
    "\n",
    "For the assigment, instead of getting a one-hot vector for each word, the neural network will get a `batch_size x num_words x 100` tensor containing floating point values.\n",
    "\n",
    "Download the GLoVe embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "editable": false,
    "id": "7zIXPea4pZCi",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "editable": false,
    "id": "-_CNat0Qpf_b",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "VOCAB_SIZE = len(glove_vectors.vectors)\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "jRcY0APp-9YI",
    "tags": []
   },
   "source": [
    "This function will embed the dataset into sequences of 100-dimension vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "editable": false,
    "id": "Yo6wUdb9s97G",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "MAX_LEN = 50\n",
    "\n",
    "def get_glove_seq(review, max_len):\n",
    "  seq = np.zeros((max_len, 100))\n",
    "  for i, word in enumerate(review):\n",
    "    if i < max_len and word in glove_vectors:\n",
    "      seq[i] = glove_vectors[word]\n",
    "  return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "editable": false,
    "id": "ZtpvG4mJnwMf",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3972493a3f420ca0978f90cacf0688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b1e6720b294c2fa7719be2a74b6b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48b16139c9542cd87e048e477e4e143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d6dd0431ad4291926742a70ab09dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199822d170d04cd9b5689642f08d8584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news_data_train = load_dataset(\"ag_news\", split=\"train\").shuffle()\n",
    "news_data_test = load_dataset(\"ag_news\", split=\"test\").shuffle()\n",
    "NEWS_TRAIN = pd.DataFrame(news_data_train)[:5000]\n",
    "NEWS_TEST = pd.DataFrame(news_data_test)[:5000]\n",
    "NUM_LABELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "editable": false,
    "id": "pmx1OElZaAki",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California Official Rules on Gay Marriage SAN ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CarMax Earnings Fall, Stock Up on Outlook (Reu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>London to unveil 2012 bid plans London #39;s b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Terms of Endearment   Seems that the Bush admi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soldiers face death after refusing to bomb Dar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  California Official Rules on Gay Marriage SAN ...      0\n",
       "1  CarMax Earnings Fall, Stock Up on Outlook (Reu...      2\n",
       "2  London to unveil 2012 bid plans London #39;s b...      1\n",
       "3  Terms of Endearment   Seems that the Bush admi...      0\n",
       "4  Soldiers face death after refusing to bomb Dar...      0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEWS_TEST.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "e5IzjEK0Ixxl",
    "tags": []
   },
   "source": [
    "Train/Test Sets using GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "editable": false,
    "id": "-I8PSBk-I6qM",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Vectorize text reviews to numbers\n",
    "X_NEWS_TRAIN = np.zeros((len(NEWS_TRAIN), MAX_LEN, 100))\n",
    "X_NEWS_TEST = np.zeros((len(NEWS_TEST), MAX_LEN, 100))\n",
    "\n",
    "for i, row in NEWS_TRAIN.iterrows():\n",
    "  X_NEWS_TRAIN[i] = get_glove_seq(tokenize_and_clean(row['text'], stem_and_remove_stop_words=False), MAX_LEN)\n",
    "\n",
    "for i, row in NEWS_TEST.iterrows():\n",
    "  X_NEWS_TEST[i] = get_glove_seq(tokenize_and_clean(row['text'], stem_and_remove_stop_words=False), MAX_LEN)\n",
    "\n",
    "Y_NEWS_TRAIN = np.array(NEWS_TRAIN['label'])\n",
    "Y_NEWS_TEST = np.array(NEWS_TEST['label'])\n",
    "NUM_LABELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "editable": true,
    "id": "i1-mK1-EPbQL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64776db849e2cffa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Defining neural network structure\n",
    "class MultinomialBoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "  def __init__(self, max_word_len, embedding_dim, num_labels):\n",
    "    super(MultinomialBoWClassifier, self).__init__()\n",
    "    self.max_word_len = max_word_len\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.num_labels = num_labels\n",
    "    \n",
    "    # Fully connected layer\n",
    "    self.fc1 = nn.Linear(embedding_dim, num_labels)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = None\n",
    "    \n",
    "    # Sum of all embedding across words\n",
    "    x = torch.mean(x, dim = 1)\n",
    "\n",
    "    # Let data through layer\n",
    "    out = self.fc1(x)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "editable": false,
    "id": "_eVavHp7TOxv",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "multibow_model = MultinomialBoWClassifier(max_word_len=MAX_LEN, embedding_dim=EMBEDDING_DIM, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "editable": false,
    "id": "wc0JD91dP9-a",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, x_train_data, y_train_data, epochs, batch_size, lr, weight_decay):\n",
    "  print('Training Started!')\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  n_iter = len(x_train_data) // batch_size\n",
    "  print(n_iter, 'batches per epoch')\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    num_correct = 0\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for i in range(n_iter):\n",
    "      x, y = get_batch(i, batch_size, x_train_data, y_train_data)\n",
    "      x = x\n",
    "      y = y.long()\n",
    "\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      total_loss += loss\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if (epoch+1)%10 == 0 and (i+1) == n_iter:\n",
    "        print('epoch:', epoch+1,',loss =',loss.item(), ', training accuracy =',(y_hat.argmax(dim=1)==y).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "editable": true,
    "id": "POVLJlC0_mEl",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# It's ok to modify this cell.\n",
    "BATCH_SIZE = 10\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 2e-3\n",
    "WEIGHT_DECAY = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "editable": false,
    "id": "uhVQB2RCbPj7",
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "500 batches per epoch\n",
      "epoch: 10 ,loss = 0.6035422682762146 , training accuracy = 0.699999988079071\n",
      "epoch: 20 ,loss = 0.5999406576156616 , training accuracy = 0.699999988079071\n",
      "epoch: 30 ,loss = 0.599229097366333 , training accuracy = 0.699999988079071\n",
      "epoch: 40 ,loss = 0.5990428328514099 , training accuracy = 0.699999988079071\n",
      "epoch: 50 ,loss = 0.5989871025085449 , training accuracy = 0.699999988079071\n",
      "epoch: 60 ,loss = 0.5989691615104675 , training accuracy = 0.699999988079071\n",
      "epoch: 70 ,loss = 0.5989631414413452 , training accuracy = 0.699999988079071\n",
      "epoch: 80 ,loss = 0.5989614129066467 , training accuracy = 0.699999988079071\n",
      "epoch: 90 ,loss = 0.5989605784416199 , training accuracy = 0.699999988079071\n",
      "epoch: 100 ,loss = 0.598960280418396 , training accuracy = 0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(multibow_model, X_NEWS_TRAIN, Y_NEWS_TRAIN, N_EPOCHS, BATCH_SIZE, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "except:\n",
    "    print(\"Training failed. Please check your code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "50_KqANXCtbs",
    "tags": []
   },
   "source": [
    "# Multinomial Regression - Test (40 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "editable": false,
    "id": "Ltz2NZ_4u9Cr",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8569999933242798\n",
      "Test G: 40/40\n"
     ]
    }
   ],
   "source": [
    "# student check - accuracies >= 80% will receive full credit (no credit for less than 80%)\n",
    "ag.test_model_accuracy_mr(X_NEWS_TEST, Y_NEWS_TEST, multibow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "hKEYvlbhS7n9",
    "tags": []
   },
   "source": [
    "# Grading\n",
    "\n",
    "Please submit this .ipynb file to Gradescope for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "id": "yGJoiI-m681a",
    "tags": []
   },
   "source": [
    "## Final Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "editable": false,
    "id": "r5xdXQJy681a",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your projected points for this assignment is 200/100.\n",
      "\n",
      "NOTE: THIS IS NOT YOUR FINAL GRADE. YOUR FINAL GRADE FOR THIS ASSIGNMENT WILL BE AT LEAST 200 OR MORE, BUT NOT LESS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# student check\n",
    "ag.FINAL_GRADE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "id": "3QabthAf681b",
    "tags": []
   },
   "source": [
    "## Notebook Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "editable": false,
    "id": "lZGz8usd681b",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook execution time in minutes = 129.86418159405392\n",
      "WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\n"
     ]
    }
   ],
   "source": [
    "# end time - notebook execution\n",
    "end_nb = time.time()\n",
    "# print notebook execution time in minutes\n",
    "print(\"Notebook execution time in minutes =\", (end_nb - start_nb)/60)\n",
    "# warn student if notebook execution time is greater than 30 minutes\n",
    "if (end_nb - start_nb)/60 > 30:\n",
    "  print(\"WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "captumWidgetMessage": {},
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1TsKPn8cwghhFR-TwD0la25YRRzhw4XF9",
     "timestamp": 1685998969758
    }
   ]
  },
  "dataExplorerConfig": {},
  "kernelspec": {
   "display_name": "cs7650_HW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "last_base_url": "https://11238.od.fbinfra.net/",
  "last_kernel_id": "8832c409-9272-44ac-a889-c233991c3bb0",
  "last_msg_id": "354bb3e2-f350a4bd64779ec7f6ce4382_4332",
  "last_server_session_id": "5625d4d2-18a1-473d-bf89-0f1a5ecc7c7b",
  "outputWidgetContext": {}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
